{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "medical bigbird.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNHWEgDXkqdesDt3SuyfqAl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cheto01/nlp-in-python-tutorial/blob/master/medical_bigbird.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VCzjc_9dEzB",
        "outputId": "85bb4f86-12b9-405a-90ba-1805ee7e0759"
      },
      "source": [
        "!pip install transformers~=4.6.0\n",
        "!pip list | grep -E 'transformers|tokenizers'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers~=4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers~=4.6.0) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers~=4.6.0) (4.5.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 20.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers~=4.6.0) (3.0.12)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers~=4.6.0) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers~=4.6.0) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers~=4.6.0) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 33.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers~=4.6.0) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers~=4.6.0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers~=4.6.0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers~=4.6.0) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers~=4.6.0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers~=4.6.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers~=4.6.0) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers~=4.6.0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers~=4.6.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers~=4.6.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers~=4.6.0) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, sacremoses, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n",
            "tokenizers                    0.10.3             \n",
            "transformers                  4.6.1              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvjFLKKXc1Xu"
      },
      "source": [
        "# 1.Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYFNd1C5b6ix",
        "outputId": "48dca1bd-fb30-4460-b27b-dc47325eaa5c"
      },
      "source": [
        "!wget -c https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt\n",
        "#!wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-16 09:50:03--  https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt\n",
            "Resolving cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)... 13.225.93.114, 13.225.93.14, 13.225.93.108, ...\n",
            "Connecting to cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)|13.225.93.114|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 312733741 (298M) [text/plain]\n",
            "Saving to: ‘oscar.eo.txt’\n",
            "\n",
            "oscar.eo.txt        100%[===================>] 298.25M  10.9MB/s    in 27s     \n",
            "\n",
            "2021-06-16 09:50:32 (11.0 MB/s) - ‘oscar.eo.txt’ saved [312733741/312733741]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F2eqMuvc-ws"
      },
      "source": [
        "# Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im_klCi3dUoc"
      },
      "source": [
        "## 1. Tokenizer\n",
        "\n",
        "Here I will use 2 different tokenizer to compare the results:\n",
        "\n",
        "*   First a byte level tokenizer trained from scratch \n",
        "*   second a BigBird tokenizer, similar borrowed from Roberta.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxWKMEPxjUTH"
      },
      "source": [
        "### 1. ByteLevel tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k8_TLYkf3EU",
        "outputId": "60335172-ff3a-4813-d75a-281486d2ab3f"
      },
      "source": [
        "%%time \n",
        "from pathlib import Path\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=paths, vocab_size=20358, min_frequency=5, special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 18min 18s, sys: 3.71 s, total: 18min 21s\n",
            "Wall time: 9min 29s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XEByBopngsO",
        "outputId": "5713c259-b9fc-474c-c55c-4484989aeac4"
      },
      "source": [
        "!mkdir med_bigbird\n",
        "tokenizer.save_model(\"med_bigbird\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['med_bigbird/vocab.json', 'med_bigbird/merges.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7Em601iy1iZ"
      },
      "source": [
        "### Sentencepiece\n",
        "\n",
        "This is required if we want to train our tokenizer and save it into a format that will be supported by the model: [spm](https://colab.research.google.com/github/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb#scrollTo=SUcAbKnRVAv6)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzVicnp4zFhG",
        "outputId": "7dbb497a-8d16-4395-dd59-074d4c6c22bf"
      },
      "source": [
        "!pip install sentencepiece\n",
        "import sentencepiece as spm\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4awJmsXHy5My",
        "outputId": "cb2a77bb-5d9e-41af-b726-c61c93fa67c5"
      },
      "source": [
        "\n",
        "## Example of user defined symbols\n",
        "spm.SentencePieceTrainer.train('--input=oscar.eo.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')\n",
        "\n",
        "sp_user = spm.SentencePieceProcessor()\n",
        "sp_user.load('m_user.model')\n",
        "\n",
        "# ids are reserved in both mode.\n",
        "# <unk>=0, <s>=1, </s>=2, <sep>=3, <cls>=4\n",
        "# user defined symbols allow these symbol to apper in the text.\n",
        "print(sp_user.encode_as_pieces('this is a test<sep> hello world<cls>'))\n",
        "print(sp_user.piece_to_id('<sep>'))  # 3\n",
        "print(sp_user.piece_to_id('<cls>'))  # 4\n",
        "print('3=', sp_user.decode_ids([3]))  # decoded to <sep>\n",
        "print('4=', sp_user.decode_ids([4]))  # decoded to <cls>"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁', 'th', 'is', '▁', 'is', '▁a', '▁te', 'st', '<sep>', '▁he', 'll', 'o', '▁', 'w', 'or', 'ld', '<cls>']\n",
            "3\n",
            "4\n",
            "3= <sep>\n",
            "4= <cls>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfPsIVlNnemY"
      },
      "source": [
        "Save the tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEkv7JI9jd4O"
      },
      "source": [
        "### 2. BigBird tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er9S0p3rQ3Y4"
      },
      "source": [
        "#@markdown don't need it for now\n",
        "from transformers import BigBirdTokenizer, PreTrainedTokenizer\n",
        "tokenizer2= BigBirdTokenizer.from_pretrained('google/bigbird-roberta-base')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfmStO2YnZoT"
      },
      "source": [
        "How to use the new tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB8utc2KnYom"
      },
      "source": [
        "#@markdown don't need it for now\n",
        "#this is how to load a tokenizer stored on disk\n",
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    \"./med_bigbird/vocab.json\",\n",
        "    \"./med_bigbird/merges.txt\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxD3kHRr5jxi",
        "outputId": "59621d8f-342f-462d-94a5-c111a578f8a0"
      },
      "source": [
        "# new tokenizer\n",
        "tokenizer.encode(\"Mi estas Julien.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=5, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wM4mowL5ru9"
      },
      "source": [
        "#bigbird tokenizer\n",
        "tokenizer2.encode(\"Mi estas Julien.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTUt4Iaf55bF"
      },
      "source": [
        "# Train the language model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arwBxMLU7bJX",
        "outputId": "8416097f-4c44-4f89-df4b-d68cd0e3a847"
      },
      "source": [
        "# Check that PyTorch sees it\n",
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g314x2Me7nDJ"
      },
      "source": [
        "### model configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQA22Rxa7l__"
      },
      "source": [
        "from transformers import BigBirdConfig\n",
        "\n",
        "config= BigBirdConfig(\n",
        "    vocab_size = 20358,\n",
        "    hidden_size=768, \n",
        "    num_hidden_layers=12, \n",
        "    num_attention_heads=12, \n",
        "    intermediate_size=3072, \n",
        "    hidden_act='gelu_fast', \n",
        "    hidden_dropout_prob=0.1, \n",
        "    attention_probs_dropout_prob=0.1, \n",
        "    max_position_embeddings=1024, \n",
        "    type_vocab_size=2, \n",
        "    initializer_range=0.02, \n",
        "    layer_norm_eps=1e-12, \n",
        "    use_cache=True, \n",
        "    is_encoder_decoder=False, \n",
        "    pad_token_id=1, \n",
        "    bos_token_id=0, \n",
        "    eos_token_id=2, \n",
        "    sep_token_id=66, \n",
        "    attention_type='block_sparse', \n",
        "    use_bias=True, \n",
        "    rescale_embeddings=False, \n",
        "    block_size=800, \n",
        "    num_random_blocks=3, \n",
        "    gradient_checkpointing=True,\n",
        "    \n",
        "    \n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xCZqY35DsuH"
      },
      "source": [
        "### create our tokenizer in transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXLytoiwEzNJ"
      },
      "source": [
        "\n",
        "from transformers import BigBirdTokenizerFast\n",
        "\n",
        "tokenizer = BigBirdTokenizerFast.from_pretrained(\"m_user.model\", max_length=1024) # generally, med_bigbird has a .spm extension\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoxznHfNvcxk"
      },
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"./med_bigbird\", max_len=1024)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVVXE_ezGHKP"
      },
      "source": [
        "### Initialize the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCSnWgGHGOxo"
      },
      "source": [
        "Here, It would be interesting to compare a pretraining from existing checkpoint weight, or just from nothing\n",
        "For the time beeing, I would like to try a training from scratch since the vocabulary is also new.\n",
        "\n",
        "The code bellow doesn't load any weight, for the future, I will try to instantiate apretrained model by loading the BigBird checkpoint `from_pretrained()` [guide](https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.from_pretrained)\n",
        "\n",
        "It could be something like\n",
        "\n",
        "`model = BigBirdForMaskedLM.from_pretrained('google/bigbird-roberta-base', other parameters) `"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZQuy7HgGKn3"
      },
      "source": [
        "from transformers import BigBirdForMaskedLM\n",
        "\n",
        "model = BigBirdForMaskedLM(config=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srYYr3PiJmnO",
        "outputId": "2fadccdf-c903-4c39-90ab-fff443e3d857"
      },
      "source": [
        "# checking the parameter size\n",
        "model.num_parameters()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102681990"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WpSW4sBJuPf"
      },
      "source": [
        "### Make our dataset ready for mlm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9J6eqWgJtsN",
        "outputId": "24c4e0be-643b-47c9-a559-f5a085fda225"
      },
      "source": [
        "%%time\n",
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"./oscar.eo.txt\",\n",
        "    block_size=800,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg3UPNLCJ_Y7"
      },
      "source": [
        "Set the mlm parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NewvclKKJLC"
      },
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycR7h_t_LlGi"
      },
      "source": [
        "### Create a trainer function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPnaeV7iLn07"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./med_bigbird\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=2,\n",
        "    #per_gpu_train_batch_size=64,\n",
        "    per_device_train_batch_size=32,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQTq1YPJL7Qc"
      },
      "source": [
        "## Training time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "mUKnYT45L_CV",
        "outputId": "dd1f1ac9-6098-4d4f-b009-09de820e877f"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention type 'block_sparse' is not possible if sequence_length: 32 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3.Changing attention type to 'original_full'...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='134' max='60910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  134/60910 2:13:44 < 1026:21:03, 0.02 it/s, Epoch 0.00/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='323' max='60910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  323/60910 5:26:22 < 1026:42:24, 0.02 it/s, Epoch 0.01/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTXpxmPPMlO9"
      },
      "source": [
        "def train_bert(train_iter, net, loss, vocab_size, devices, num_steps):\n",
        "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
        "    trainer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
        "    step, timer = 0, d2l.Timer()\n",
        "    animator = d2l.Animator(xlabel='step', ylabel='loss', xlim=[1, num_steps],\n",
        "                            legend=['mlm', 'nsp'])\n",
        "    # Sum of masked language modeling losses, sum of next sentence prediction\n",
        "    # losses, no. of sentence pairs, count\n",
        "    metric = d2l.Accumulator(4)\n",
        "    num_steps_reached = False\n",
        "    while step < num_steps and not num_steps_reached:\n",
        "        for tokens_X, segments_X, valid_lens_x, pred_positions_X,\\\n",
        "            mlm_weights_X, mlm_Y, nsp_y in train_iter:\n",
        "            tokens_X = tokens_X.to(devices[0])\n",
        "            segments_X = segments_X.to(devices[0])\n",
        "            valid_lens_x = valid_lens_x.to(devices[0])\n",
        "            pred_positions_X = pred_positions_X.to(devices[0])\n",
        "            mlm_weights_X = mlm_weights_X.to(devices[0])\n",
        "            mlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0])\n",
        "            trainer.zero_grad()\n",
        "            timer.start()\n",
        "            mlm_l, nsp_l, l = _get_batch_loss_bert(\n",
        "                net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,\n",
        "                pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)\n",
        "            l.backward()\n",
        "            trainer.step()\n",
        "            metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)\n",
        "            timer.stop()\n",
        "            animator.add(step + 1,\n",
        "                         (metric[0] / metric[3], metric[1] / metric[3]))\n",
        "            step += 1\n",
        "            if step == num_steps:\n",
        "                num_steps_reached = True\n",
        "                break\n",
        "\n",
        "    print(f'MLM loss {metric[0] / metric[3]:.3f}, '\n",
        "          f'NSP loss {metric[1] / metric[3]:.3f}')\n",
        "    print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on '\n",
        "          f'{str(devices)}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}